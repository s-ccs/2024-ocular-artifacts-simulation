
@article{gawne_effect_2017,
	title = {The effect of saccadic eye movements on the sensor-level magnetoencephalogram},
	volume = {128},
	issn = {1388-2457},
	url = {https://www.sciencedirect.com/science/article/pii/S1388245716310252},
	doi = {10.1016/j.clinph.2016.12.013},
	abstract = {Objective
We used a combination of simulation and recordings from human subjects to characterize how saccadic eye movements affect the magnetoencephalogram (MEG).
Methods
We used simulated saccadic eye movements to generate simulated MEG signals. We also recorded the MEG signals from three healthy adults to 5° magnitude saccades that were vertical up and down, and horizontal left and right.
Results
The signal elicited by the rotating eye dipoles is highly dependent on saccade direction, can cover a large area, can sometimes have a non-intuitive trajectory, but does not significantly extend above approximately 30Hz in the frequency domain. In contrast, the saccadic spikes (which are primarily monophasic pulses, but can be biphasic) are highly localized to the lateral frontal regions for all saccade directions, and in the frequency domain extend up past 60Hz.
Conclusions
Gamma band saccadic artifact is spatially localized to small regions regardless of saccade direction, but beta band and lower frequency saccadic artifact have broader spatial extents that vary strongly as a function of saccade direction.
Significance
We have here characterized the MEG saccadic artifact in both the spatial and the frequency domains for saccades of different directions. This could be important in ruling in or ruling out artifact in MEG recordings.},
	number = {3},
	urldate = {2024-08-12},
	journal = {Clinical Neurophysiology},
	author = {Gawne, Timothy J. and Killen, Jeffrey F. and Tracy, John M. and Lahti, Adrienne C.},
	month = mar,
	year = {2017},
	keywords = {Gamma band, Magnetoencephalography, MEG, Saccade, Saccadic spike, Saccadic spike field},
	pages = {397--407},
	file = {Gawne et al. - 2017 - The effect of saccadic eye movements on the sensor.pdf:/home/mnk/Zotero/storage/GT98F598/Gawne et al. - 2017 - The effect of saccadic eye movements on the sensor.pdf:application/pdf;ScienceDirect Snapshot:/home/mnk/Zotero/storage/X89ECMBK/S1388245716310252.html:text/html},
}

@article{barbara_real-time_2023,
	title = {Real-time continuous {EOG}-based gaze angle estimation with baseline drift compensation under stationary head conditions},
	volume = {86},
	issn = {17468094},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1746809423007152},
	doi = {10.1016/j.bspc.2023.105282},
	abstract = {Objective: This work aims to propose a novel method to estimate the gaze from electrooculography (EOG) signals while compensating for the baseline drift, and which intrinsically detects fixations, saccades and blinks. In contrast to existing baseline drift mitigation techniques, the proposed framework is real-time-implementable, and does not require the average point of gaze to lie at the primary gaze position, nor does it disrupt the overall ocular pose-induced EOG signal DC characteristics. The EOG data used to validate the proposed method is also being made publicly available.
Methods: The proposed method is based on the dual Kalman filter, which estimates the gaze angles (GAs) and the baseline concurrently, taking into consideration the EOG signal’s non-stationary and temporally-multimodal characteristics. In fact, it is a multiple-model technique based on a battery model of the eye wherein fixations, saccades and blinks are modelled separately.
Results: When applied to short EOG data segments, a horizontal and vertical GA estimation error of 1.64 ± 0.82◦ and 1.97 ± 0.34◦, respectively, was obtained, which compared well with the corresponding results obtained using the state-of-the-art linear regression models. Conversely, for longer data segments, the proposed framework yielded superior GA estimation performance when compared to the state-of-the-art techniques. Eye movement detection and labelling F-scores exceeding 90\% were achieved.
Conclusion: The proposed method yields reliable gaze estimation performance, and accurately detects fixations, saccades and blinks. Significance: This work proposes an integrated method to simultaneously estimate the GAs and address the baseline drift issue without the limitations of existing baseline drift mitigation techniques.},
	language = {en},
	urldate = {2024-09-03},
	journal = {Biomedical Signal Processing and Control},
	author = {Barbara, Nathaniel and Camilleri, Tracey A. and Camilleri, Kenneth P.},
	month = sep,
	year = {2023},
	pages = {105282},
	file = {PDF:/home/mnk/Zotero/storage/JE95YJ5A/Barbara et al. - 2023 - Real-time continuous EOG-based gaze angle estimation with baseline drift compensation under stationa.pdf:application/pdf},
}

@article{harmening_hartmutmodeling_2022,
	title = {{HArtMuT}—modeling eye and muscle contributors in neuroelectric imaging},
	volume = {19},
	issn = {1741-2560, 1741-2552},
	url = {https://iopscience.iop.org/article/10.1088/1741-2552/aca8ce},
	doi = {10.1088/1741-2552/aca8ce},
	abstract = {Objective. Magneto- and electroencephalography (M/EEG) measurements record a mix of signals from the brain, eyes, and muscles. These signals can be disentangled for artifact cleaning e.g. using spatial filtering techniques. However, correctly localizing and identifying these components relies on head models that so far only take brain sources into account. Approach. We thus developed the Head Artifact Model using Tripoles (HArtMuT). This volume conduction head model extends to the neck and includes brain sources as well as sources representing eyes and muscles that can be modeled as single dipoles, symmetrical dipoles, and tripoles. We compared a HArtMuT four-layer boundary element model (BEM) with the EEGLAB standard head model on their localization accuracy and residual variance (RV) using a HArtMuT finite element model (FEM) as ground truth. We also evaluated the RV on real-world data of mobile participants, comparing different HArtMuT BEM types with the EEGLAB standard head model. Main results. We found that HArtMuT improves localization for all sources, especially non-brain, and localization error and RV of non-brain sources were in the same range as those of brain sources. The best results were achieved by using cortical dipoles, muscular tripoles, and ocular symmetric dipoles, but dipolar sources alone can already lead to convincing results. Significance. We conclude that HArtMuT is well suited for modeling eye and muscle contributions to the M/EEG signal. It can be used to localize sources and to identify brain, eye, and muscle components. HArtMuT is freely available and can be integrated into standard software.},
	language = {en},
	number = {6},
	urldate = {2024-09-04},
	journal = {Journal of Neural Engineering},
	author = {Harmening, Nils and Klug, Marius and Gramann, Klaus and Miklody, Daniel},
	month = dec,
	year = {2022},
	pages = {066041},
	file = {PDF:/home/mnk/Zotero/storage/XH5LU5RJ/Harmening et al. - 2022 - HArtMuT—modeling eye and muscle contributors in neuroelectric imaging.pdf:application/pdf},
}

@article{berg_dipole_1991,
	title = {Dipole models of eye movements and blinks},
	volume = {79},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00134694},
	url = {https://linkinghub.elsevier.com/retrieve/pii/001346949190154V},
	doi = {10.1016/0013-4694(91)90154-V},
	language = {en},
	number = {1},
	urldate = {2024-09-04},
	journal = {Electroencephalography and Clinical Neurophysiology},
	author = {Berg, Patrick and Scherg, Michael},
	month = jul,
	year = {1991},
	pages = {36--44},
	file = {Berg und Scherg - 1991 - Dipole models of eye movements and blinks.pdf:/home/mnk/Zotero/storage/DA8JBS2V/Berg und Scherg - 1991 - Dipole models of eye movements and blinks.pdf:application/pdf},
}

@article{karas_brain-computer_2023,
	title = {Brain-computer interface for robot control with eye artifacts for assistive applications},
	volume = {13},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-44645-y},
	doi = {10.1038/s41598-023-44645-y},
	abstract = {Abstract
            Human-robot interaction is a rapidly developing field and robots have been taking more active roles in our daily lives. Patient care is one of the fields in which robots are becoming more present, especially for people with disabilities. People with neurodegenerative disorders might not consciously or voluntarily produce movements other than those involving the eyes or eyelids. In this context, Brain-Computer Interface (BCI) systems present an alternative way to communicate or interact with the external world. In order to improve the lives of people with disabilities, this paper presents a novel BCI to control an assistive robot with user’s eye artifacts. In this study, eye artifacts that contaminate the electroencephalogram (EEG) signals are considered a valuable source of information thanks to their high signal-to-noise ratio and intentional generation. The proposed methodology detects eye artifacts from EEG signals through characteristic shapes that occur during the events. The lateral movements are distinguished by their ordered peak and valley formation and the opposite phase of the signals measured at F7 and F8 channels. This work, as far as the authors’ knowledge, is the first method that used this behavior to detect lateral eye movements. For the blinks detection, a double-thresholding method is proposed by the authors to catch both weak blinks as well as regular ones, differentiating itself from the other algorithms in the literature that normally use only one threshold. Real-time detected events with their virtual time stamps are fed into a second algorithm, to further distinguish between double and quadruple blinks from single blinks occurrence frequency. After testing the algorithm offline and in realtime, the algorithm is implemented on the device. The created BCI was used to control an assistive robot through a graphical user interface. The validation experiments including 5 participants prove that the developed BCI is able to control the robot.},
	language = {en},
	number = {1},
	urldate = {2024-09-12},
	journal = {Scientific Reports},
	author = {Karas, Kaan and Pozzi, Luca and Pedrocchi, Alessandra and Braghin, Francesco and Roveda, Loris},
	month = oct,
	year = {2023},
	pages = {17512},
	file = {PDF:/home/mnk/Zotero/storage/A4IUBDFE/Karas et al. - 2023 - Brain-computer interface for robot control with eye artifacts for assistive applications.pdf:application/pdf},
}

@article{iwasaki_effects_2005,
	title = {Effects of eyelid closure, blinks, and eye movements on the electroencephalogram},
	volume = {116},
	issn = {1388-2457},
	url = {https://www.sciencedirect.com/science/article/pii/S138824570400416X},
	doi = {https://doi.org/10.1016/j.clinph.2004.11.001},
	abstract = {Objective To characterize the effects of the eyeball and eyelid positions during eyeblinks on electroencephalographic (EEG) potentials. Methods Movements of the upper eyelids and eyes were measured in two healthy subjects using the magnetic search coil technique during horizontal and vertical eye rotations, eyeblinks, and lid closure. Corresponding signal changes were recorded simultaneously on the electroencephalogram (EEG). Results Spontaneous blinks produced small eye movements directed down and inward, whereas slow or forced blinks were associated with delayed upward eye rotations (i.e. Bell's phenomenon); both types of blinks caused positive EEG potentials with bifrontal distribution maximum at Fp1 and Fp2. Conclusions In prior reports, these positive EEG artifacts have been attributed to upward eyeball rotation during blinks—Bell's phenomenon. By contrast, our findings indicate that movements of the eyelid contribute to a greater extent to these EEG potentials than do upward eyeball rotations. Significance Care is required in attributing EEG artifacts to movements of either eyeball or eyelid, since our findings suggest that they both contribute to these potentials.},
	number = {4},
	journal = {Clinical Neurophysiology},
	author = {Iwasaki, Masaki and Kellinghaus, Christoph and Alexopoulos, Andreas V. and Burgess, Richard C. and Kumar, Arun N. and Han, Yanning H. and Lüders, Hans O. and Leigh, R. John},
	year = {2005},
	keywords = {Artifacts, Blinks, Electroencephalogram, Eye movements, Eyelid, Saccades},
	pages = {878--885},
	file = {ScienceDirect Full Text PDF:/home/mnk/Zotero/storage/7NFS3KET/Iwasaki et al. - 2005 - Effects of eyelid closure, blinks, and eye movements on the electroencephalogram.pdf:application/pdf},
}

@article{nystrom_what_2024,
	title = {What is a blink? {Classifying} and characterizing blinks in eye openness signals},
	volume = {56},
	issn = {1554-3528},
	shorttitle = {What is a blink?},
	url = {https://link.springer.com/10.3758/s13428-023-02333-9},
	doi = {10.3758/s13428-023-02333-9},
	abstract = {Blinks, the closing and opening of the eyelids, are used in a wide array of ﬁelds where human function and behavior are studied. In data from video-based eye trackers, blink rate and duration are often estimated from the pupil-size signal. However, blinks and their parameters can be estimated only indirectly from this signal, since it does not explicitly contain information about the eyelid position. We ask whether blinks detected from an eye openness signal that estimates the distance between the eyelids (EO blinks) are comparable to blinks detected with a traditional algorithm using the pupil-size signal (PS blinks) and how robust blink detection is when data quality is low. In terms of rate, there was an almost-perfect overlap between EO and PS blink (F1 score: 0.98) when the head was in the center of the eye tracker’s tracking range where data quality was high and a high overlap (F1 score 0.94) when the head was at the edge of the tracking range where data quality was worse. When there was a difference in blink rate between EO and PS blinks, it was mainly due to data loss in the pupil-size signal. Blink durations were about 60 ms longer in EO blinks compared to PS blinks. Moreover, the dynamics of EO blinks was similar to results from previous literature. We conclude that the eye openness signal together with our proposed blink detection algorithm provides an advantageous method to detect and describe blinks in greater detail.},
	language = {en},
	number = {4},
	urldate = {2024-09-12},
	journal = {Behavior Research Methods},
	author = {Nyström, Marcus and Andersson, Richard and Niehorster, Diederick C. and Hessels, Roy S. and Hooge, Ignace T. C.},
	month = feb,
	year = {2024},
	pages = {3280--3299},
	file = {PDF:/home/mnk/Zotero/storage/QTQDS7VR/Nyström et al. - 2024 - What is a blink Classifying and characterizing blinks in eye openness signals.pdf:application/pdf},
}

@article{kleifges_blinker_2017,
	title = {{BLINKER}: {Automated} {Extraction} of {Ocular} {Indices} from {EEG} {Enabling} {Large}-{Scale} {Analysis}},
	volume = {11},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2017.00012},
	doi = {10.3389/fnins.2017.00012},
	abstract = {{\textless}p{\textgreater}Electroencephalography (EEG) offers a platform for studying the relationships between behavioral measures, such as blink rate and duration, with neural correlates of fatigue and attention, such as theta and alpha band power. Further, the existence of EEG studies covering a variety of subjects and tasks provides opportunities for the community to better characterize variability of these measures across tasks and subjects. We have implemented an automated pipeline (BLINKER) for extracting ocular indices such as blink rate, blink duration, and blink velocity-amplitude ratios from EEG channels, EOG channels, and/or independent components (ICs). To illustrate the use of our approach, we have applied the pipeline to a large corpus of EEG data (comprising more than 2000 datasets acquired at eight different laboratories) in order to characterize variability of certain ocular indicators across subjects. We also investigate dependence of ocular indices on task in a shooter study. We have implemented our algorithms in a freely available MATLAB toolbox called BLINKER. The toolbox, which is easy to use and can be applied to collections of data without user intervention, can automatically discover which channels or ICs capture blinks. The tools extract blinks, calculate common ocular indices, generate a report for each dataset, dump labeled images of the individual blinks, and provide summary statistics across collections. Users can run BLINKER as a script or as a plugin for EEGLAB. The toolbox is available at {\textless}ext-link ext-link-type="uri" xlink:href="https://github.com/VisLab/EEG-Blinks" xmlns:xlink="http://www.w3.org/1999/xlink"{\textgreater}https://github.com/VisLab/EEG-Blinks{\textless}/ext-link{\textgreater}. User documentation and examples appear at {\textless}ext-link ext-link-type="uri" xlink:href="http://vislab.github.io/EEG-Blinks/" xmlns:xlink="http://www.w3.org/1999/xlink"{\textgreater}http://vislab.github.io/EEG-Blinks/{\textless}/ext-link{\textgreater}.{\textless}/p{\textgreater}},
	journal = {Frontiers in Neuroscience},
	author = {Kleifges, Kelly and Bigdely-Shamlo, Nima and Kerick, Scott E. and Robbins, Kay A.},
	year = {2017},
	file = {Full Text PDF:/home/mnk/Zotero/storage/XSIU7L66/Kleifges et al. - 2017 - BLINKER Automated Extraction of Ocular Indices from EEG Enabling Large-Scale Analysis.pdf:application/pdf},
}

@book{malmivuo_bioelectromagnetismprinciples_1995,
	title = {{BioelectromagnetismPrinciples} and {Applications} of {Bioelectric} and {Biomagnetic} {Fields}},
	isbn = {978-0-19-505823-9},
	url = {https://academic.oup.com/book/25966},
	language = {en},
	urldate = {2024-09-12},
	publisher = {Oxford University Press},
	author = {Malmivuo, Jaakko and Plonsey, Robert},
	month = oct,
	year = {1995},
	doi = {10.1093/acprof:oso/9780195058239.001.0001},
	doi = {10.1093/acprof:oso/9780195058239.001.0001},
	file = {PDF:/home/mnk/Zotero/storage/7SMH9EUA/Malmivuo and Plonsey - 1995 - BioelectromagnetismPrinciples and Applications of Bioelectric and Biomagnetic Fields.pdf:application/pdf},
}

@incollection{hutchison_study_2005,
	address = {Berlin, Heidelberg},
	title = {Study on the {Velocity} of {Saccadic} {Eye} {Movements}},
	volume = {3681},
	isbn = {978-3-540-28894-7 978-3-540-31983-2},
	url = {http://link.springer.com/10.1007/11552413_115},
	abstract = {The velocity of eye movements can be measured by using the corneo-retinal potential of the eyeball. This potential is changed almost in proportion to the eccentricity of the eyeball, and we analyze the detailed motion of the eyeball. Particularly, we measured the velocity of the horizontal saccadic eye movements. Then, we obtained the useful results by investigating experimentally the velocity difference between left eyeball and right eyeball in such following cases. When both eyeballs are opened, the velocity of the adduction is faster than the velocity of the abduction. When one eyeball is covered, the velocity of the covered eyeball is faster than the velocity of the opened eyeball.},
	language = {en},
	urldate = {2024-09-16},
	booktitle = {Knowledge-{Based} {Intelligent} {Information} and {Engineering} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sasaki, Hiroshi and Ishii, Naohiro},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Khosla, Rajiv and Howlett, Robert J. and Jain, Lakhmi C.},
	year = {2005},
	doi = {10.1007/11552413_115},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {808--812},
	file = {PDF:/home/mnk/Zotero/storage/R28GFXY5/Sasaki and Ishii - 2005 - Study on the Velocity of Saccadic Eye Movements.pdf:application/pdf},
}

@article{kierkels_model-based_2006,
	title = {A {Model}-{Based} {Objective} {Evaluation} of {Eye} {Movement} {Correction} in {EEG} {Recordings}},
	volume = {53},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9294},
	url = {http://ieeexplore.ieee.org/document/1580830/},
	doi = {10.1109/TBME.2005.862533},
	language = {en},
	number = {2},
	urldate = {2024-09-16},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Kierkels, J.J.M. and vanBoxtel, G.J.M. and Vogten, L.L.M.},
	month = feb,
	year = {2006},
	pages = {246--253},
	file = {Full Text PDF:/home/mnk/Zotero/storage/RLZH59VW/Kierkels et al. - 2006 - A Model-Based Objective Evaluation of Eye Movement Correction in EEG Recordings.pdf:application/pdf},
}

@article{berg_dipole_1991-1,
	title = {Dipole modelling of eye activity and its application to the removal of eye artefacts from the {EEG} and {MEG}},
	volume = {12},
	issn = {0143-0815},
	url = {https://iopscience.iop.org/article/10.1088/0143-0815/12/A/010},
	doi = {10.1088/0143-0815/12/a/010},
	abstract = {The spatio-temporal dipole model approach has been used to identify the difference dipoles arising from changes in the ocular dipoles due to eye movements and blinks. Based on these results a method has been developed to remove eye artefacts from electrical or magneric data. T h e method avoids distortions due to the head model by determining the spatial distribution of the signals from the eyes empirically. Using simultaneous modelling of the BBC or MBG activity with dipole sources distributed within the head together with the empirically determined spatial eye components, the eye activity can be estimated and removed from the eec or MEG. This greatly reduces the distortion to the topography that is a concomitant of previous eye artefact correction methods. T h e advantages of the merhod are illustrated using simulated and real electricnl dam.},
	language = {en},
	number = {A},
	urldate = {2024-09-16},
	journal = {Clinical Physics and Physiological Measurement},
	author = {Berg, P and Scherg, M},
	month = jan,
	year = {1991},
	note = {Publisher: IOP Publishing},
	pages = {49--54},
	file = {PDF:/home/mnk/Zotero/storage/67D75KDW/Berg and Scherg - 1991 - Dipole modelling of eye activity and its application to the removal of eye artefacts from the EEG an.pdf:application/pdf},
}

@article{elbert_removal_1985,
	title = {Removal of ocular artifacts from the {EEG} — {A} biophysical approach to the {EOG}},
	volume = {60},
	issn = {0013-4694},
	url = {https://www.sciencedirect.com/science/article/pii/001346948591020X},
	doi = {https://doi.org/10.1016/0013-4694(85)91020-X},
	abstract = {The present paper describes the propagation of ocular potentials across the scalp on a biophysical basis. It is concluded that 3 EOG derivations (two for EEG records along the midline) are generally necessary to account for ocular disturbances in the EEG. The inadequacy of many methods suggested for EOG artifact control may be due to the false assumption that just one EOG derivation provides enough information to remove ocular potentials from any EEG recording along the mid(-sagittal) line. A comparison of compensation with one or with two EOG derivations is described for a data set of slow brain potentials. A frequency dependence of the ocular influence cannot be neglected, if fast and slow EOG activities have to be removed. The present considerations should allow a more theoretically based decision of the EOG correction method necessary for a certain data set. Résumé Cet article décrit la propagation des potentiels oculaires sur le scalp à l'aide de bases biophysiques. La conclusion en est que 3 dérivations EOG (deux pour les enregistrements EEG sur la ligne médiane) sont généralement nécessaires pour tenir compte des perturbations oculaires dans l'EEG. Que des méthodes suggérées pour le contrôle des artéfacts EOG soient inadéquates peut être dû à la fausse supposition qu'il suffit d'une électrode de dérivation EOG pour fournir assez d'informations utiles à l'extraction des potentiels oculaires de n'importe quel enregistrement EEG pratiqué sur la ligne médiane. Une comparaison de compensation avec une ou deux dérivations EOG est décrite pour une série de données de potentiels lents cérébraux. On ne peut négligé une dépendance de la fréquence des influences oculaires si les activités EOG rapides et lentes ont été extraites. Ces présentes considérations permettraient une décision plus théoriquement basée de la méthode de correction nécessaire pour une certaine série de données.},
	number = {5},
	journal = {Electroencephalography and Clinical Neurophysiology},
	author = {Elbert, Thomas and Lutzenberger, Werner and Rockstroh, Brigitte and Birbaumer, Niels},
	year = {1985},
	pages = {455--463},
	file = {PDF:/home/mnk/Zotero/storage/GANN3Z7N/Elbert et al. - 1985 - Removal of ocular artifacts from the EEG — A biophysical approach to the EOG.pdf:application/pdf},
}

@article{barea_wheelchair_nodate,
	title = {Wheelchair {Guidance} {Strategies} {Using} {EOG}},
	abstract = {This paper describes an eye-control method, based on electrooculography (EOG), for guiding and controlling a wheelchair for disabled people; the control is actually effected by eye movements within the socket. An eye model based on an electrooculographic signal is proposed and its validity is studied. Different techniques and guidance strategies are then shown with comments on the advantages and disadvantages of each one. The system consists of a standard electric wheelchair with an on-board computer, sensors and a graphic user interface run by the computer. This control technique could be useful in multiple applications, such as mobility and communication aid for handicapped persons.},
	language = {en},
	author = {Barea, R and Boquete, L and Mazo, M and López, E},
	file = {PDF:/home/mnk/Zotero/storage/FDKN92KK/Barea et al. - Wheelchair Guidance Strategies Using EOG.pdf:application/pdf},
}

@article{picton_correction_2000,
	title = {The correction of ocular artifacts: a topographic perspective},
	volume = {111},
	issn = {1388-2457},
	shorttitle = {The correction of ocular artifacts},
	url = {https://www.sciencedirect.com/science/article/pii/S1388245799002278},
	doi = {10.1016/S1388-2457(99)00227-8},
	abstract = {Objective: To evaluate the scalp topography of the potentials related to saccades and blinks. Methods: The scalp topographies of the potentials associated with saccades and blinks were recorded in 60 subjects. The topographies were analyzed using both source components and attenuation factors, with each factor representing the fraction of the potential recorded in peri-ocular electrodes that contributes to the EEG recorded from a particular scalp location. Results: Blinks and upward saccades generated potentials with very different topographies. Left and right saccades and up and down saccades generated equal but inverted fields except at peri-ocular locations where subtle inequalities occurred. The potentials associated with lateral saccades were consistently larger in female subjects than in male subjects. Conclusions: The differences in the scalp topographies between blinks and vertical saccades can be explained by the different ways in which they are generated. Blink potentials are caused by the eyelids sliding down over the positively charged cornea, whereas saccade potentials are caused by changes in the orientation of the corneoretinal dipole. Any compensation procedure for ocular artifacts must take into account the topographic differences between blinks and upward saccades.},
	number = {1},
	urldate = {2024-09-18},
	journal = {Clinical Neurophysiology},
	author = {Picton, Terence W and van Roon, Patricia and Armilio, Maria L and Berg, Patrick and Ille, Nicole and Scherg, Michael},
	month = jan,
	year = {2000},
	keywords = {EEG artifacts, Electro-oculogram, Scalp topography},
	pages = {53--65},
	file = {ScienceDirect Full Text PDF:/home/mnk/Zotero/storage/N92X3E3Q/Picton et al. - 2000 - The correction of ocular artifacts a topographic perspective.pdf:application/pdf;ScienceDirect Snapshot:/home/mnk/Zotero/storage/W2TL5PZY/S1388245799002278.html:text/html},
}

@article{lins_ocular_1993,
	title = {Ocular artifacts in recording {EEGs} and event-related potentials {II}: {Source} dipoles and source components},
	volume = {6},
	copyright = {http://www.springer.com/tdm},
	issn = {0896-0267, 1573-6792},
	shorttitle = {Ocular artifacts in recording {EEGs} and event-related potentials {II}},
	url = {http://link.springer.com/10.1007/BF01234128},
	doi = {10.1007/BF01234128},
	language = {en},
	number = {1},
	urldate = {2024-09-18},
	journal = {Brain Topography},
	author = {Lins, Otavio G. and Picton, Terence W. and Berg, Patrick and Scherg, Michael},
	month = sep,
	year = {1993},
	pages = {65--78},
	file = {Full Text PDF:/home/mnk/Zotero/storage/WKPUNHUP/Lins et al. - 1993 - Ocular artifacts in recording EEGs and event-related potentials II Source dipoles and source compon.pdf:application/pdf},
}

@article{ai_direction_2016,
	title = {Direction and viewing area-sensitive influence of {EOG} artifacts revealed in the {EEG} topographic pattern analysis},
	volume = {10},
	issn = {1871-4080, 1871-4099},
	url = {http://link.springer.com/10.1007/s11571-016-9382-4},
	doi = {10.1007/s11571-016-9382-4},
	language = {en},
	number = {4},
	urldate = {2024-09-18},
	journal = {Cognitive Neurodynamics},
	author = {Ai, Guangyi and Sato, Naoyuki and Singh, Balbir and Wagatsuma, Hiroaki},
	month = aug,
	year = {2016},
	pages = {301--314},
	file = {PDF:/home/mnk/Zotero/storage/KCL53SF8/Ai et al. - 2016 - Direction and viewing area-sensitive influence of EOG artifacts revealed in the EEG topographic patt.pdf:application/pdf},
}

@article{jia_measurement_2019,
	title = {Measurement of saccadic eye movements by electrooculography for simultaneous {EEG} recording},
	volume = {51},
	issn = {1554-3528},
	url = {http://link.springer.com/10.3758/s13428-019-01280-8},
	doi = {10.3758/s13428-019-01280-8},
	abstract = {Eye movements are an important index of the neural functions of visual information processing, decision making, visuomotor coordination, sports performance, and so forth. However, the available optical tracking methods are impractical in many situations, such as the wearing of eyeglasses or the presence of ophthalmic disorders, and this can be overcome by accurate recording of eye movements by electrooculography (EOG). In this study we recorded eye movements by EOG simultaneously with highdensity electroencephalogram (EEG) recording using a 128-channel EGI electrode net at a 500-Hz sampling rate, including appropriate facial electrodes. The participants made eye movements over a calibration target consisting of a 5×5 grid of stimulus targets. The results showed that the EOG methodology allowed accurate analysis of the amplitude and direction of the fixation locations and saccadic dynamics with a temporal resolution of 500 Hz, under both cued and uncued analysis regimes. Blink responses could be identified separately and were shown to have a more complex source derivation than has previously been recognized. The results also showed that the EOG signals recorded through the EEG net can achieve results as accurate as typical optical eye-tracking devices, and also allow for simultaneous assessment of neural activity during all types of eye movements. Moreover, the EOG method effectively avoids the technical difficulties related to eye-tracker positioning and the synchronization between EEG and eye movements. We showed that simultaneous EOG/EEG recording is a convenient means of measuring eye movements, with an accuracy comparable to that of many specialized eye-tracking systems.},
	language = {en},
	number = {5},
	urldate = {2024-09-18},
	journal = {Behavior Research Methods},
	author = {Jia, Yingxin and Tyler, Christopher W.},
	month = oct,
	year = {2019},
	pages = {2139--2151},
	file = {PDF:/home/mnk/Zotero/storage/PD9ITWKW/Jia and Tyler - 2019 - Measurement of saccadic eye movements by electrooculography for simultaneous EEG recording.pdf:application/pdf},
}

@article{barea_eog_2000,
	title = {{EOG} {Technique} to guide a wheelchair},
	abstract = {This paper involves a method to control and guide a wheelchair for disabled people using electrooculography (EOG), so that control is made by means of the ocular position (eye displacement into its orbit). An eye model based on electrooculographic signal is proposed and its validity is studied. After that, differents tecniques of guidance are shown and advantages and disadvanges of each one are commented. The system consists of a standard electric wheelchair with an on board computer, sensors and graphics user interface running on the computer [1]. This control technique can be useful in multiple applications, such as help to the movility and the comunication between handicapped persons.},
	author = {Barea, Rafael and Boquete, L and Mazo, Manuel and López, E and García-Lledó, Alberto},
	month = jan,
	year = {2000},
	file = {Full Text PDF:/home/mnk/Zotero/storage/V3GA5ZXN/Barea et al. - 2000 - EOG Technique to guide a wheelchair.pdf:application/pdf},
}

@inproceedings{barbara_modelling_2021,
	address = {Virtual Event Germany},
	title = {Modelling of {Blink}-{Related} {Eyelid}-{Induced} {Shunting} on the {Electrooculogram}},
	isbn = {978-1-4503-8345-5},
	url = {https://dl.acm.org/doi/10.1145/3448018.3457994},
	doi = {10.1145/3448018.3457994},
	abstract = {Besides the traditional regression model-based techniques to estimate the gaze angles (GAs) from electrooculography (EOG) signals, more recent works have investigated the use of a battery model for GA estimation. This is a white-box, explicit and physically-driven model which relates the monopolar EOG potential to the electrodecornea and electrode-retina distances. In this work, this model is augmented to cater for the blink-induced EOG signal characteristics, by modelling the eyelid-induced shunting effect during blinks. Specifically, a channel-dependent parameter representing the extent to which the amount of eyelid opening affects the particular EOG channel is introduced. A method to estimate these parameters is also proposed and the proposed model is validated by incorporating it in a Kalman filter to estimate the eyelid opening during blinks. The results obtained have demonstrated that the proposed model can accurately represent the blink-related eyelid-induced shunting.},
	language = {en},
	urldate = {2024-09-23},
	booktitle = {{ACM} {Symposium} on {Eye} {Tracking} {Research} and {Applications}},
	publisher = {ACM},
	author = {Barbara, Nathaniel and Camilleri, Tracey A. and Camilleri, Kenneth P.},
	month = may,
	year = {2021},
	pages = {1--6},
	file = {PDF:/home/mnk/Zotero/storage/KF27CBLF/Barbara et al. - 2021 - Modelling of Blink-Related Eyelid-Induced Shunting on the Electrooculogram.pdf:application/pdf},
}

@article{barbara_monopolar_2023,
	title = {Monopolar and bipolar electrooculography signal characteristics due to target displacements—have we seen the whole picture?},
	volume = {44},
	issn = {0967-3334, 1361-6579},
	url = {https://iopscience.iop.org/article/10.1088/1361-6579/acb03d},
	doi = {10.1088/1361-6579/acb03d},
	abstract = {The development of electrooculography (EOG)-based human-computer interface systems is generally based on the processing of the commonly referred to horizontal and vertical bipolar EOG channels, which are computed from a horizontally-aligned and another vertically-aligned pair of electrodes, respectively. Horizontal (vertical) target displacements are assumed to result in changes in the horizontal (vertical) EOG channel only, and any cross-talk between the bipolar channels is often neglected or incorrectly attributed solely to electrode misalignment with respect to the ocular rotation axes. Objective. The aim of this work is to demonstrate that such cross-talk is intrinsic to the geometric relationship between the orientation of the verging ocular globes and the planar displacement of the gaze target with respect to the primary gaze position. Approach. Since it is difﬁcult to record actual EOG data with electrodes which are perfectly-aligned with the ocular rotation axes, this is studied by simulating the EOG potential values for various horizontally- and vertically-displacing targets using a dipole model of the eye. Main results. We show that cross-talk between the horizontal and vertical bipolar EOG channels is manifested even if the electrodes are aligned with the ocular rotation axes. Speciﬁcally, for a horizontally- (vertically-)displaced target, while the monopolar EOG signals obtained from the horizontally- (vertically-)aligned electrodes exhibit an expected predominant potential displacement, a smaller displacement is also exhibited in the monopolar EOG signals obtained from the vertically- (horizontally-)aligned electrodes. These unexpected displacements in the vertically- (horizontally-)aligned monopolar channels may have different magnitudes, resulting in an effective potential displacement in the vertical (horizontal) bipolar EOG channel. Signiﬁcance. This is signiﬁcant as it shows that, unlike in many works published so far for EOG-based ocular pose estimation, it is not sufﬁcient to only use the horizontal (vertical) bipolar EOG channel to estimate the horizontal (vertical) displacement of the ocular pose.},
	language = {en},
	number = {3},
	urldate = {2024-09-23},
	journal = {Physiological Measurement},
	author = {Barbara, Nathaniel and Camilleri, Tracey A and Camilleri, Kenneth P},
	month = mar,
	year = {2023},
	pages = {035011},
	file = {PDF:/home/mnk/Zotero/storage/22997IKU/Barbara et al. - 2023 - Monopolar and bipolar electrooculography signal characteristics due to target displacements—have we.pdf:application/pdf},
}

@article{keren_saccadic_2010,
	title = {Saccadic spike potentials in gamma-band {EEG}: {Characterization}, detection and suppression},
	volume = {49},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811909011288},
	doi = {https://doi.org/10.1016/j.neuroimage.2009.10.057},
	abstract = {Analysis of high-frequency (gamma-band) neural activity by means of non-invasive EEG is gaining increasing interest. However, we have recently shown that a saccade-related spike potential (SP) seriously confounds the analysis of EEG induced gamma-band responses (iGBR), as the SP eludes traditional EEG artifact rejection methods. Here we provide a comprehensive profile of the SP and evaluate methods for its detection and suppression, aiming to unveil true cerebral gamma-band activity. The SP appears consistently as a sharp biphasic deflection of about 22 ms starting at the saccade onset, with a frequency band of ∼20–90 Hz. On the average, larger saccades elicit higher SP amplitudes. The SP amplitude gradually changes from the extra-ocular channels towards posterior sites with the steepest gradients around the eyes, indicating its ocular source. Although the amplitude and the sign of the SP depend on the choice of reference channel, the potential gradients remain the same and non-zero for all references. The scalp topography is modulated almost exclusively by the direction of saccades, with steeper gradients ipsilateral to the saccade target. We discuss how the above characteristics impede attempts to remove these SPs from the EEG by common temporal filtering, choice of different references, or rejection of contaminated trials. We examine the extent to which SPs can be reliably detected without an eye tracker, assess the degree to which scalp current density derivation attenuates the effect of the SP, and propose a tailored ICA procedure for minimizing the effect of the SP.},
	number = {3},
	journal = {NeuroImage},
	author = {Keren, Alon S. and Yuval-Greenberg, Shlomit and Deouell, Leon Y.},
	year = {2010},
	pages = {2248--2263},
	file = {PDF:/home/mnk/Zotero/storage/5NCF5B68/Keren et al. - 2010 - Saccadic spike potentials in gamma-band EEG Characterization, detection and suppression.pdf:application/pdf},
}

@article{mowrer_corneo-retinal_1935,
	title = {{THE} {CORNEO}-{RETINAL} {POTENTIAL} {DIFFERENCE} {AS} {THE} {BASIS} {OF} {THE} {GALVANOMETRIC} {METHOD} {OF} {RECORDING} {EYE} {MOVEMENTS}},
	volume = {114},
	issn = {0002-9513},
	url = {https://www.physiology.org/doi/10.1152/ajplegacy.1935.114.2.423},
	doi = {10.1152/ajplegacy.1935.114.2.423},
	language = {en},
	number = {2},
	urldate = {2024-09-23},
	journal = {American Journal of Physiology-Legacy Content},
	author = {Mowrer, O. H. and Ruch, T. C. and Miller, N. E.},
	month = dec,
	year = {1935},
	pages = {423--428},
	file = {PDF:/home/mnk/Zotero/storage/S5VFJRU8/Mowrer et al. - 1935 - THE CORNEO-RETINAL POTENTIAL DIFFERENCE AS THE BASIS OF THE GALVANOMETRIC METHOD OF RECORDING EYE MO.pdf:application/pdf},
}

@article{riemslag_origin_1988,
	title = {On the origin of the presaccadic spike potential},
	volume = {70},
	issn = {0013-4694},
	url = {https://www.sciencedirect.com/science/article/pii/0013469488900466},
	doi = {https://doi.org/10.1016/0013-4694(88)90046-6},
	abstract = {The spike potential (SP) accompanying the onset of saccadic eye movements has been reported to originate near the orbital region. Its dependence on saccade size, however, does not correlate with the behaviour of any of the possible sources of the potential available in the orbital region. The exact size dependence cannot be studied fro results obtained with classical ENG methods to detect the saccadic onset and furthermore without removal of the artefact in the signal caused by the corneo-retinal potential. We recorded the SP by monitoring the eye movements with an infrared scleral reflection method (IRIS), and carefully studied the SP amplitude as a function of saccade size, and revealed a more realistic function. Furthermore, removal of the artefact of the corneo-retinal potential revealed a biphasic wave shape of the SP instead of the usually observed monophasic peak. These results support the hypothesis that this electrical activity accompanying the onset of saccadic eye movements originates in the oculomotor neurones innervating the ocular muscle units.},
	number = {4},
	journal = {Electroencephalography and Clinical Neurophysiology},
	author = {Riemslag, F. C. C. and Heijde, G. L. Van der and Dongen, M. M. M. M. Van and Ottenhoff, F.},
	year = {1988},
	keywords = {Saccades, Eye movement evoked potentials, Spike potential},
	pages = {281--287},
}

@article{lins_ocular_1993-1,
	title = {Ocular artifacts in {EEG} and event-related potentials {I}: {Scalp} topography},
	volume = {6},
	copyright = {http://www.springer.com/tdm},
	issn = {0896-0267, 1573-6792},
	shorttitle = {Ocular artifacts in {EEG} and event-related potentials {I}},
	url = {http://link.springer.com/10.1007/BF01234127},
	doi = {10.1007/BF01234127},
	language = {en},
	number = {1},
	urldate = {2024-10-30},
	journal = {Brain Topography},
	author = {Lins, Otavio G. and Picton, Terence W. and Berg, Patrick and Scherg, Michael},
	month = sep,
	year = {1993},
	pages = {51--63},
	file = {PDF:/home/mnk/Zotero/storage/NUE7KCEA/Lins et al. - 1993 - Ocular artifacts in EEG and event-related potentials I Scalp topography.pdf:application/pdf},
}

@article{plochl_combining_2012,
	title = {Combining {EEG} and eye tracking: identification, characterization, and correction of eye movement artifacts in electroencephalographic data},
	volume = {6},
	issn = {1662-5161},
	shorttitle = {Combining {EEG} and eye tracking},
	url = {https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2012.00278/full},
	doi = {10.3389/fnhum.2012.00278},
	abstract = {{\textless}p{\textgreater}Eye movements introduce large artifacts to electroencephalographic recordings (EEG) and thus render data analysis difficult or even impossible. Trials contaminated by eye movement and blink artifacts have to be discarded, hence in standard EEG-paradigms subjects are required to fixate on the screen. To overcome this restriction, several correction methods including regression and blind source separation have been proposed. Yet, there is no automated standard procedure established. By simultaneously recording eye movements and 64-channel-EEG during a guided eye movement paradigm, we investigate and review the properties of eye movement artifacts, including corneo-retinal dipole changes, saccadic spike potentials and eyelid artifacts, and study their interrelations during different types of eye- and eyelid movements. In concordance with earlier studies our results confirm that these artifacts arise from different independent sources and that depending on electrode site, gaze direction, and choice of reference these sources contribute differently to the measured signal. We assess the respective implications for artifact correction methods and therefore compare the performance of two prominent approaches, namely linear regression and independent component analysis (ICA). We show and discuss that due to the independence of eye artifact sources, regression-based correction methods inevitably over- or under-correct individual artifact components, while ICA is in principle suited to address such mixtures of different types of artifacts. Finally, we propose an algorithm, which uses eye tracker information to objectively identify eye-artifact related ICA-components (ICs) in an automated manner. In the data presented here, the algorithm performed very similar to human experts when those were given both, the topographies of the ICs and their respective activations in a large amount of trials. Moreover it performed more reliable and almost twice as effective than human experts when those had to base their decision on IC topographies only. Furthermore, a receiver operating characteristic (ROC) analysis demonstrated an optimal balance of false positive and false negative at an area under curve (AUC) of more than 0.99. Removing the automatically detected ICs from the data resulted in removal or substantial suppression of ocular artifacts including microsaccadic spike potentials, while the relevant neural signal remained unaffected. In conclusion the present work aims at a better understanding of individual eye movement artifacts, their interrelations and the respective implications for eye artifact correction. Additionally, the proposed ICA-procedure provides a tool for optimized detection and correction of eye movement-related artifact components.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-12-20},
	journal = {Frontiers in Human Neuroscience},
	author = {Plöchl, Michael and Ossandón, José Pablo and König, Peter},
	month = oct,
	year = {2012},
	note = {Publisher: Frontiers},
	keywords = {Artifact correction, EEG, Eye Movements, eye tracking, independent component analysis (ICA), regression},
	file = {Full Text PDF:/home/mnk/Zotero/storage/UBUAUN9B/Plöchl et al. - 2012 - Combining EEG and eye tracking identification, characterization, and correction of eye movement art.pdf:application/pdf},
}

@article{bulling_eye_2011,
	title = {Eye {Movement} {Analysis} for {Activity} {Recognition} {Using} {Electrooculography}},
	volume = {33},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/5444879/},
	doi = {10.1109/TPAMI.2010.86},
	abstract = {In this work, we investigate eye movement analysis as a new sensing modality for activity recognition. Eye movement data were recorded using an electrooculography (EOG) system. We first describe and evaluate algorithms for detecting three eye movement characteristics from EOG signals—saccades, fixations, and blinks—and propose a method for assessing repetitive patterns of eye movements. We then devise 90 different features based on these characteristics and select a subset of them using minimum redundancy maximum relevance (mRMR) feature selection. We validate the method using an eight participant study in an office environment using an example set of five activity classes: copying a text, reading a printed paper, taking handwritten notes, watching a video, and browsing the Web. We also include periods with no specific activity (the NULL class). Using a support vector machine (SVM) classifier and person-independent (leave-one-person-out) training, we obtain an average precision of 76.1 percent and recall of 70.5 percent over all classes and participants. The work demonstrates the promise of eye-based activity recognition (EAR) and opens up discussion on the wider applicability of EAR to other activities that are difficult, or even impossible, to detect using common sensing modalities.},
	language = {en},
	number = {4},
	urldate = {2025-01-19},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bulling, Andreas and Ward, Jamie A and Gellersen, Hans and Tröster, Gerhard},
	month = apr,
	year = {2011},
	pages = {741--753},
	file = {PDF:/home/mnk/Zotero/storage/38TL5GT9/Bulling et al. - 2011 - Eye Movement Analysis for Activity Recognition Using Electrooculography.pdf:application/pdf},
}

@article{wolters_efficient_2004,
	title = {Efficient computation of lead field bases and influence matrix for the {FEM}-based {EEG} and {MEG} inverse problem},
	volume = {20},
	issn = {0266-5611, 1361-6420},
	url = {https://iopscience.iop.org/article/10.1088/0266-5611/20/4/007},
	doi = {10.1088/0266-5611/20/4/007},
	abstract = {The inverse problem in electro- and magneto-encephalography (EEG/MEG) aims at reconstructing the underlying current distribution in the human brain using potential differences and/or magnetic ﬂuxes that are measured noninvasively directly, or at a close distance, from the head surface. The simulation of EEG and MEG ﬁelds for a given dipolar source in the brain using a volumeconduction model of the head is called the forward problem. The ﬁnite element (FE) method, used for the forward problem, is able to realistically model tissue conductivity inhomogeneities and anisotropies, which is crucial for an accurate reconstruction of the current distribution. So far, the computational complexity is quite large when using the necessary high resolution FE models. In this paper we will extend the concept of the EEG lead ﬁeld basis to the MEG and present algorithms for their efﬁcient computation. Exploiting the fact that the number of sensors is generally much smaller than the number of reasonable dipolar sources, our lead ﬁeld approach will speed up the state-of-the-art forward approach by a factor of more than 100 for a realistic choice of the number of sensors and sources. Our approaches can be applied to inverse reconstruction algorithms in both continuous and discrete source parameter space for EEG and MEG. In combination with algebraic multigrid solvers, the presented approach leads to a highly efﬁcient solution of FE-based source reconstruction problems.},
	language = {en},
	number = {4},
	urldate = {2025-01-21},
	journal = {Inverse Problems},
	author = {Wolters, C H and Grasedyck, L and Hackbusch, W},
	month = aug,
	year = {2004},
	pages = {1099--1116},
	file = {PDF:/home/mnk/Zotero/storage/HJE8G8FZ/Wolters et al. - 2004 - Efficient computation of lead field bases and influence matrix for the FEM-based EEG and MEG inverse.pdf:application/pdf},
}

@inproceedings{estrany_human_2022,
	address = {Cham},
	title = {Human {Eye} {Tracking} {Through} {Electro}-{Oculography} ({EOG}): {A} {Review}},
	isbn = {978-3-031-16538-2},
	shorttitle = {Human {Eye} {Tracking} {Through} {Electro}-{Oculography} ({EOG})},
	doi = {10.1007/978-3-031-16538-2_8},
	abstract = {The basic principles and techniques used in Electrooculography (EOG) are presented. The main objective of this work is to present a state of art of Electrooculography (EOG) in Human computer Interface (HCI) to help researchers interested in the field.},
	language = {en},
	booktitle = {Cooperative {Design}, {Visualization}, and {Engineering}},
	publisher = {Springer International Publishing},
	author = {Estrany, B. and Fuster-Parra, Pilar},
	editor = {Luo, Yuhua},
	year = {2022},
	keywords = {Saccade, Eye movements, Electroencephalography (EEG), Electrooculography (EOG), Eye tracking, Gaze estimation, Human-computer interface (HCI), Nonlinearity, eog},
	pages = {75--85},
	file = {Full Text PDF:/home/mnk/Zotero/storage/6L6KP84W/Estrany and Fuster-Parra - 2022 - Human Eye Tracking Through Electro-Oculography (EOG) A Review.pdf:application/pdf},
}

@article{chaumon_practical_2015,
	title = {A practical guide to the selection of independent components of the electroencephalogram for artifact correction},
	volume = {250},
	issn = {01650270},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165027015000928},
	doi = {10.1016/j.jneumeth.2015.02.025},
	language = {en},
	urldate = {2025-01-30},
	journal = {Journal of Neuroscience Methods},
	author = {Chaumon, Maximilien and Bishop, Dorothy V.M. and Busch, Niko A.},
	month = jul,
	year = {2015},
	pages = {47--63},
	file = {PDF:/home/mnk/Zotero/storage/2KTC5TN7/Chaumon et al. - 2015 - A practical guide to the selection of independent components of the electroencephalogram for artifac.pdf:application/pdf},
}

@article{moon_positional_2020,
	title = {Positional {Change} of the {Eyeball} {During} {Eye} {Movements}: {Evidence} of {Translatory} {Movement}},
	volume = {11},
	issn = {1664-2295},
	shorttitle = {Positional {Change} of the {Eyeball} {During} {Eye} {Movements}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7527524/},
	doi = {10.3389/fneur.2020.556441},
	abstract = {Purpose: To investigate the positional change of the eyeball induced by horizontal and vertical gazing to deduce translatory movement, using three-dimensional (3D) magnetic resonance imaging (MRI)., Methods: In this prospective observational study participants underwent orbital MRI during central, right, left, up, and down gazing. MRI scans were processed using self-developed software; this software enabled 3D MR image reconstruction and the superimposition of reconstructed image sets between different gazes. After acquiring the coordinates of the eyeball centroid in each gaze, the changes in centroid coordinates from central gaze to the other gazes were estimated, and correlations with associated factors were evaluated., Results: The mean distance of centroid movement was 0.69 ± 0.27 mm in abduction, 0.68 ± 0.27 mm in adduction, 0.43 ± 0.23 mm in elevation, and 0.44 ± 0.19 mm in depression. The mean angle of centroid movement in horizontal gaze, measured in terms of the movement of the left eye centroid in the axial plane, was 228.7° in abduction and −4.2° in adduction. In vertical gaze, the mean angle of centroid movement was −96.8° in elevation and 101.8° in depression. Axial length and ocular volume were negatively correlated with the distance of centroid movement in horizontal gaze., Conclusions: The position of the eyeball moved in the same direction as the gaze during horizontal gaze, but in the opposite direction during vertical gaze. For accurate eye movement analyses, such as the measurement of the deviation angle in strabismus, translation should be considered in addition to rotation.},
	urldate = {2025-02-01},
	journal = {Frontiers in Neurology},
	author = {Moon, Yeji and Lee, Won June and Shin, Seung Hak and Kim, Ji Hong and Lee, Ji Young and Oh, Sei Yeul and Lim, Han Woong},
	month = sep,
	year = {2020},
	pmid = {33041994},
	pmcid = {PMC7527524},
	pages = {556441},
	file = {PubMed Central Full Text PDF:/home/mnk/Zotero/storage/CBADE4S6/Moon et al. - 2020 - Positional Change of the Eyeball During Eye Movements Evidence of Translatory Movement.pdf:application/pdf},
}

@book{carpenter_movements_nodate,
	title = {Movements of the {Eyes}},
	url = {http://wexler.free.fr/library/files/carpenter%20(1988)%20movements%20of%20the%20eyes.pdf},
	author = {Carpenter, R. H. S.},
}

@article{matsuo_electrical_1975,
	title = {Electrical phenomena associated with movements of the eyelid},
	volume = {38},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00134694},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0013469475901911},
	doi = {10.1016/0013-4694(75)90191-1},
	language = {en},
	number = {5},
	urldate = {2025-02-23},
	journal = {Electroencephalography and Clinical Neurophysiology},
	author = {Matsuo, Fumisuke and Peters, Jon F and Reilly, Edward L},
	month = may,
	year = {1975},
	pages = {507--511},
}
@software{schepers_2025_14894630,
  author       = {Schepers, Judith and
                  Lips, Luis and
                  Marathe, Maanik and
                  Ehinger, Benedikt},
  title        = {UnfoldSim.jl: Simulating continuous event-based
                   time series data for EEG and beyond
                  },
  month        = feb,
  year         = 2025,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.14894630},
  url          = {https://doi.org/10.5281/zenodo.14894630},
  swhid        = {swh:1:dir:701a4809f473feec4e6c9eba506a5e118e2a2006
                   ;origin=https://doi.org/10.5281/zenodo.7738651;vis
                   it=swh:1:snp:8a176996650906f784375a11d6a9bcfe7685c
                   be5;anchor=swh:1:rel:327abcd3cc72fbff347d629e43d36
                   f43b2374865;path=unfoldtoolbox-
                   UnfoldSim.jl-33db9c1
                  },
}

@book{hari_meg-eeg_2017,
	address = {Cary},
	title = {{MEG}-{EEG} {Primer}},
	isbn = {978-0-19-049778-1},
	url = {https://ebookcentral.proquest.com/lib/uni-stuttgart/detail.action?docID=5746005},
	language = {eng},
	publisher = {Oxford University Press USA - OSO},
	author = {Hari, Riitta},
	collaborator = {Puce, Aina},
	year = {2017},
}

@inproceedings{jas:hal-01313458,
  TITLE = {{Automated rejection and repair of bad trials in MEG/EEG}},
  AUTHOR = {Jas, Mainak and Engemann, Denis and Raimondo, Federico and Bekhti, Yousra and Gramfort, Alexandre},
  URL = {https://hal.science/hal-01313458},
  BOOKTITLE = {{6th International Workshop on Pattern Recognition in Neuroimaging (PRNI)}},
  ADDRESS = {Trento, Italy},
  YEAR = {2016},
  MONTH = Jun,
  KEYWORDS = {magnetoencephalography ; electroencephalography ; preprocessing ; artifact rejection ; automation ; machine learning},
  PDF = {https://hal.science/hal-01313458v1/file/automated-rejection-repair.pdf},
  HAL_ID = {hal-01313458},
  HAL_VERSION = {v1},
}

@article{delorme_eeglab_2004,
	title = {{EEGLAB}: an open source toolbox for analysis of single-trial {EEG} dynamics including independent component analysis},
	volume = {134},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01650270},
	shorttitle = {{EEGLAB}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165027003003479},
	doi = {10.1016/j.jneumeth.2003.10.009},
	language = {en},
	number = {1},
	urldate = {2025-03-08},
	journal = {Journal of Neuroscience Methods},
	author = {Delorme, Arnaud and Makeig, Scott},
	month = mar,
	year = {2004},
	pages = {9--21},
	file = {Submitted Version:/home/mnk/Zotero/storage/V8T374IL/Delorme and Makeig - 2004 - EEGLAB an open source toolbox for analysis of single-trial EEG dynamics including independent compo.pdf:application/pdf},
}

@misc{larson_mne-python_2024,
	title = {{MNE}-{Python}},
	copyright = {BSD 3-Clause "New" or "Revised" License},
	url = {https://zenodo.org/doi/10.5281/zenodo.592483},
	abstract = {Full Changelog: https://github.com/mne-tools/mne-python/compare/v1.8.0...v1.9.0},
	urldate = {2025-03-08},
	publisher = {Zenodo},
	author = {Larson, Eric and Gramfort, Alexandre and Engemann, Denis A and Leppakangas, Jaakko and Brodbeck, Christian and Jas, Mainak and Brooks, Teon L and Sassenhagen, Jona and McCloy, Daniel and Luessi, Martin and King, Jean-Rémi and Höchenberger, Richard and Goj, Roman and Brunner, Clemens and Favelier, Guillaume and van Vliet, Marijn and Wronkiewicz, Mark and Rockhill, Alex and Holdgraf, Chris and Scheltienne, Mathieu and Massich, Joan and Appelhoff, Stefan and Bekhti, Yousra and Leggitt, Alan and Dykstra, Andrew and Trachel, Romain and Luke, Robert and De Santis, Lorenzo and Panda, Asish and Magnuski, Mikołaj and Westner, Britta and Wakeman, Dan G and Strohmeier, Daniel and Bharadwaj, Hari and Linzen, Tal and Barachant, Alexandre and Ruzich, Emily and Bailey, Christopher J and Li, Adam and Moutard, Clément and Bloy, Luke and Raimondo, Fede and Nurminen, Jussi and Billinger, Martin and Montoya, Jair and Woodman, Marmaduke and Huberty, Scott and Lee, Ingoo and Schulz, Martin and Foti, Nick and Nangini, Cathy and García Alanis, José C and Orfanos, Dimitri Papadopoulos and Hauk, Olaf and Maddox, Ross and LaPlante, Roan and Drew, Ashley and Dinh, Christoph and Dumas, Guillaume and Martin and Benerradi, Johann and Hartmann, Thomas and Ort, Eduard and Pasler, Paul and Repplinger, Stefan and Rudiuk, Alexander and Radanovic, Ana and Buran, Brad and Woessner, Jacob and Massias, Mathurin and Hämäläinen, Matti and Sripad, Praveen and Chirkov, Valerii and Mullins, Christopher and Raimundo, Félix and Kaneda, Michiru and Alday, Phillip and Pari, Ram and Kornblith, Simon and Halchenko, Yaroslav and Luo, Yu-Han and Kasper, Johannes and Doelling, Keith and Jensen, Mads and Gahlot, Tanay and Binns, Thomas S and Nunes, Adonay and Gütlin, Dirk and Heinila, Erkka and Armeni, Kristijan and kjs and Weinstein, Alejandro and Lamus, Camilo and Galván, Catalina María and Moënne-Loccoz, Cristóbal and Altukhov, Dmitrii and Peterson, Erica and Hanna, Jevri and Houck, Jon and Klein, Natalie and Roujansky, Paul and Luke, Rob and Ruuskanen, Santeri and Kern, Simon and Rantala, Antti and Maess, Burkhard and Forster, Carina and O'Reilly, Christian and Welke, Dominik and Kolkhorst, Henrich and Banville, Hubert and Zhang, Jack and Maksymenko, Kostiantyn and Clarke, Maggie and Anelli, Matteo and Chapochnikov, Nikolai and Bannier, Pierre-Antoine and Choudhary, Saket and Kim, Cora and Klotzsche, Felix and Wong, Fu-Te and Kojcic, Ivana and Nielsen, Jesper Duemose and Lankinen, Kaisu and Tabavi, Kambiz and Thibault, Louis and Gerster, Moritz and Alibou, Nabil and Gayraud, Nathalie and Ward, Nick and Herbst, Sophie and Férat, Victor and Quinn, Andrew and Gauthier, Antoine and Pinsard, Basile and Stephen, Emily and Hornberger, Erik and Hathaway, Evan and Kalenkovich, Evgenii and Mamashli, Fahimeh and Belonosov, Gennadiy and O'Neill, George and Marinato, Giorgio and Anevar, Hafeza and Abdelhedi, Hamza and Sosulski, Jan and Stout, Jeff and Calder-Travis, Joshua and Zhu, Judy D and Eisenman, Larry and Esch, Lorenz and Dovgialo, Marian and Barascud, Nicolas and Legrand, Nicolas and Kapralov, Nikolai and Chu, Qian and Falach, Rotem and Deslauriers-Gauthier, Samuel and Cotroneo, Silvia and Matindi, Steve and Bierer, Steven and Binns, Thomas Samuel and Stenner, Tristan and Peterson, Victoria and Baratz, Zvi and Tonin, Alessandro and Kovrig, Alexander and Pascarella, Annalisa and Karekal, Apoorva and de la Torre, Carlos and Gohil, Chetan and Zhao, Christina and Krzemiński, Dominik and Makowski, Dominique and Mikulan, Ezequiel and Hofer, Florian and Schiratti, Jean-Baptiste and Evans, Jen and Veillette, John and Drew, Jordan and Teves, Joshua and Mathewson, Kyle and Gwilliams, Laura and Varghese, Lenny and Hamilton, Liberty and Gemein, Lukas and Hecker, Lukas and Lx37 and van Es, Mats and Boggess, Matt and Eberlein, Matthias and Žák, Michal and Sherif, Mohamed and Kozhemiako, Nataliia and Srinivasan, Naveen and Wilming, Niklas and Kozynets, Oleh and Molfese, Peter J and Ablin, Pierre and Das, Proloy and Bertrand, Quentin and Shoorangiz, Reza and Scholz, Richard and Hübner, Rodrigo and Sommariva, Sara and Er, Sena and Khan, Sheraz and Datta, Sumalyo and Papadopoulo, Theodore and Donoghue, Thomas and Jochmann, Thomas and Merk, Timon and Flak, Tod and Dupré la Tour, Tom and NessAiver, Tziona and akshay0724 and sviter and Earle-Richardson, Aaron and Hindle, Abram and Koutsou, Achilleas and Fecker, Adeline and Wagner, Adina and Ciok, Alex and Lepauvre, Alex and Kiefer, Alexander and Gilbert, Andy and Pradhan, Aniket and Padee, Anna and Dubarry, Anne-Sophie and Waniek, Anton Nikolas and Singhal, Archit and Rokem, Ariel and Pelzer, Arne and Hurst, Austin and Beasley, Ben and Nicenboim, Bruno and Clauss, Christian and Mista, Christian and Li, Chun-Hui and Braboszcz, Claire and Schad, Daniel C and Hasegan, Daniel and Tse, Daniel and Sleiter, Darin Erat and Haslacher, David and Sabbagh, David and Kostas, Demetres and Petkova, Desislava and Issagaliyeva, Dinara and Das, Diptyajit and Wetzel, Dominik and Eich, Eberhard and DuPre, Elizabeth and Lau, Ellen and Olivetti, Emanuele and Varano, Enrico and Altamiranda, Enzo and Brayet, Eric and de Montalivet, Etienne and Goldstein, Evgeny and Negahbani, Farzin and Zamberlan, Federico and Pop, Florin and Weber, Frederik D and Tan, Gansheng and Brookshire, Geoff and Giulio and Reina, Gonzalo and Maymandi, Hamid and Arzoo, Hasrat Ali and Sonntag, Hermann and Ye, Hongjiang and Shin, Hyonyoung and Elmas, Hüseyin Orkun and AZZ, Ilian and Machairas, Ilias and Zubarev, Ivan and de Jong, Ivo and Phelan, Jacob and Kaczmarzyk, Jakub and Zerfowski, Jan and van den Bosch, Jasper J F and Van Der Donckt, Jeroen and van der Meer, Johan and Niediek, Johannes and Koen, Josh and Bear, Joshua J and Dammers, Juergen and Galán, Julia Guiomar Niso and Welzel, Julius and Slama, Katarina and Leinweber, Katrin and Grabot, Laetitia and Andersen, Lau Møller and Almeida, Leonardo Rochael and Barbosa, Leonardo S and Alfine, Lorenzo and Hejtmánek, Lukáš and Balatsko, Maksym and Kitzbichler, Manfred and Kumar, Manoj and Kadwani, Manorama and Sutela, Manu and Koculak, Marcin and Henney, Mark and BaBer, Martin and Oberg, Martin and van Harmelen, Martin and Courtemanche, Matt and Tucker, Matt and Visconti di Oleggio Castello, Matteo and Dold, Matthias and Toivonen, Matti and Shader, Maureen and Cespedes, Mauricio and Krause, Michael and Rybář, Milan and He, Mingjian and Daneshzand, Mohammad and Fourcaud-Trocmé, Nicolas and Gensollen, Nicolas and Proulx, Nicole and Focke, Niels and Chalas, Nikolas and Markowitz, Noah and Shubi, Omer and Mainar, Pablo and Sundaram, Padma and Silva, Pedro and Li, Quanliang and Barthélemy, Quentin and Nadkarni, Rahul and Gatti, Ramiro and Apariciogarcia, Ramonapariciog and Aagaard, Rasmus and Nasri, Reza and Koehler, Richard and Stargardsky, Riessarius and Oostenveld, Robert and Seymour, Robert and Schirrmeister, Robin Tibor and Law, Ryan and Pai, Sagun and Perry, Sam and Louviot, Samuel and Saha, Sawradip and Mathot, Sebastiaan and Major, Sebastian and Treguer, Sebastien and Castaño, Sebastián and Deng, Senwen and Antopolskiy, Sergey and Shirazi, Seyed (Yahya) and Wong, Simeon and Hofmann, Simon M and Poil, Simon-Shlomo and Foslien, Sondre and Singh, Sourav and Chambon, Stanislas and Bethard, Steven and Gutstein, Steven M and Meyer, Svea Marie and Wang, T and Moreau, Thomas and Radman, Thomas and Gates, Timothy and Ma, Tom and Stone, Tom and Clausner, Tommy and Anijärv, Toomas Erik and Kumaravel, Velu Prabhakar and Turner, Will and Zuazo, Xabier de and Xia, Xiaokai and Zuo, Yiping and Zhang, Zhi and ZENG, Ziyi and btkcodedev and buildqa and luzpaz},
	month = dec,
	year = {2024},
	doi = {10.5281/ZENODO.592483},
	keywords = {DBS, deep brain stimulation, eCoG, EEG, electrocorticography, electroencephalography, fNIRS, functional near-infrared spectroscopy, iEEG, intracranial EEG, magnetoencephalography, MEG},
}


@misc{ehinger_unfoldjl_2025,
	title = {Unfold.jl: event-related regression toolbox},
	copyright = {MIT License},
	shorttitle = {Unfold.jl},
	url = {https://zenodo.org/doi/10.5281/zenodo.14652576},
	abstract = {Unfold v0.8.1

Diff since v0.8.0

Bugfix: residuals were incorrectly calculated in the case the data is longer than the model

Merged pull requests:



Update predict.jl (\#244) (@behinger)},
	urldate = {2025-03-08},
	publisher = {Zenodo},
	author = {Ehinger, Benedikt and Alday, Phillip},
	month = jan,
	year = {2025},
	doi = {10.5281/ZENODO.14652576},
	keywords = {circular, EEG, ERP, event-related potentials, fMRI, GLM, MixedModels, non-linear, non-linear ERP, regression ERP, rERP, statistics},
}


@article{krol_sereega_2018,
	title = {{SEREEGA}: {Simulating} event-related {EEG} activity},
	volume = {309},
	issn = {01650270},
	shorttitle = {{SEREEGA}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165027018302395},
	doi = {10.1016/j.jneumeth.2018.08.001},
	language = {en},
	urldate = {2025-03-08},
	journal = {Journal of Neuroscience Methods},
	author = {Krol, Laurens R. and Pawlitzki, Juliane and Lotte, Fabien and Gramann, Klaus and Zander, Thorsten O.},
	month = nov,
	year = {2018},
	pages = {13--24},
}

@article{Schepers2025, doi = {10.21105/joss.06641}, url = {https://doi.org/10.21105/joss.06641}, year = {2025}, publisher = {The Open Journal}, volume = {10}, number = {107}, pages = {6641}, author = {Judith Schepers and Luis Lips and Maanik Marathe and Benedikt V. Ehinger}, title = {UnfoldSim.jl: Simulating continuous event-based time series data for EEG and beyond}, journal = {Journal of Open Source Software} }